---
bibliography: bib.bib
format: gfm
---
# Word embeddings - GloVE and Word2vec

The purpose of this project is to draw insights on 
GloVe 840B Common Crawl pretrained word embeddings [@penningtonGloVeGlobalVectorsdata] and
Word2Vec Google News pretrained word embeddings [@mikolovWord2vecToolComputing2013].

The structure is the following:

* `analysis.ipynb` A quarto file containing the analysis. 
* `data/` : The data folder contains only the tokens from GloVe 300 dense word embeddings pretrained on 840 billion tokens. 

## Requirements:

Downloading Peter Norvig merged sample of Google
books unigram http://norvig.com/google-books-common-words.txt

```console
wget http://norvig.com/google-books-common-words.txt -P data/
```

Tokens from Word2vec embeddings are required.
I made a repo which contains those tokens only.

They should be downloaded and move to the data folder:

```console
wget  https://raw.githubusercontent.com/sondalex/word2vec-tokens/master/word2vec_text_GloVe_format.wordsonly.txt -P data/
```

## References


